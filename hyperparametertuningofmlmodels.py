# -*- coding: utf-8 -*-
"""HyperparameterTuningofMLModels.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YmVj-BfcqQGUnJB4PIUQlYi09nsNkW53
"""

# Import necessary libraries
import pandas as pd
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from scipy.stats import randint
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Load data from Google Drive
file_path = '/content/drive/MyDrive/emails.csv (1).zip'  # Update with your file path
data = pd.read_csv(file_path)

# Print column names and data types to identify correct names
print("Column names:", data.columns.tolist())
print("Data types:\n", data.dtypes)

# Drop columns that are non-numeric and not useful for the model
# Update 'Email' with the actual name of the email or irrelevant column if different
data = data.drop(columns=['Email No.'])  # Replace 'YourEmailColumnName' with the correct name

# Separate features and target
X = data.drop('Prediction', axis=1)  # Features
y = data['Prediction']  # Target

# Check for remaining non-numeric columns
non_numeric_cols = X.select_dtypes(include=['object']).columns
print("Non-numeric columns:", non_numeric_cols)

# Convert non-numeric columns to numeric using OneHotEncoder
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(), non_numeric_cols)
    ],
    remainder='passthrough'
)

# Define the model
model = RandomForestClassifier()

# Create a pipeline with preprocessing and model
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', model)
])

# Grid Search for hyperparameter tuning
param_grid = {
    'model__n_estimators': [100, 200],
    'model__max_depth': [10, 20],
    'model__min_samples_split': [2, 5]
}

grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, scoring='accuracy')
grid_search.fit(X, y)

# Best parameters and best score from Grid Search
print("Grid Search Best Parameters:", grid_search.best_params_)
print("Grid Search Best Score:", grid_search.best_score_)

# Evaluate the best model from Grid Search
y_pred_grid = grid_search.best_estimator_.predict(X)
print("Grid Search Accuracy:", accuracy_score(y, y_pred_grid))
print("Grid Search Classification Report:\n", classification_report(y, y_pred_grid))

# Random Search for hyperparameter tuning
param_distributions = {
    'model__n_estimators': randint(100, 500),
    'model__max_depth': randint(10, 50),
    'model__min_samples_split': randint(2, 10)
}

random_search = RandomizedSearchCV(estimator=pipeline, param_distributions=param_distributions, n_iter=50, cv=5, scoring='accuracy', random_state=42)
random_search.fit(X, y)

# Best parameters and best score from Random Search
print("Random Search Best Parameters:", random_search.best_params_)
print("Random Search Best Score:", random_search.best_score_)

# Evaluate the best model from Random Search
y_pred_random = random_search.best_estimator_.predict(X)
print("Random Search Accuracy:", accuracy_score(y, y_pred_random))
print("Random Search Classification Report:\n", classification_report(y, y_pred_random))